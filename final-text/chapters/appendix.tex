\chapter{Documentação do StarVZ sobre HDFS/Spark}

Nesta Seção serão mostrados os passos necessários para reproduzir os testes 
realizados. Também há algumas referências para o 
\href{https://github.com/aksmiyazaki/tcc-spec}{repositório deste trabalho no 
Github}, onde encontram-se os scripts de automação de grande parte do setup e 
execução dos testes.

O Setup do ambiente para executar os experimentos começa com o script 
\mytexttt{setup\_environment.py}, dentro da pasta scripts do repositório. É 
necessário copiar também o script \mytexttt{setup\_rpackages.R} para o mesmo 
diretório, pois este é chamado pelo primeiro.


Após executar o script \mytexttt{setup\_environment.py}, para finalizar 
a instalação do Hadoop é necessário:

\begin{itemize}
 \item Executar o comando \mytexttt{source $\sim$/.bashrc};
 \item Verificar se o parâmetro \mytexttt{fs.default.name} no arquivo 
\mytexttt{core-site.xml} está configurado corretamente (deve apontar para o 
\emph{Namenode});
 \item Verificar se o parâmetro \mytexttt{dfs.replication} no arquivo 
\mytexttt{hdfs-site.xml} está coerente. Nos experimentos, este foi definido 
como o número de nós;
\item Configurar o arquivo \mytexttt{slaves}, que basicamente lista todos os 
nós do cluster Hadoop.
\end{itemize}

Feito isso, é necessário realizar as configurações do Spark. Para isso, deve-se 
configurar o arquivo \mytexttt{slaves}, da mesma forma que no Hadoop. Também é 
preciso configurar os parâmetros \mytexttt{SPARK\_LOCAL\_IP} e 
\mytexttt{SPARK\_MASTER\_HOST} no arquivo \mytexttt{spark-env.sh}, com o IP 
local e o IP do nó mestre respectivamente. Revise também os parâmetros no 
arquivo \mytexttt{spark-env.sh}, pois há uma série de apontamentos para o nó 
mestre.

Agora, vamos iniciar o Hadoop. Para isso, entre na pasta raiz de sua instalação 
e execute a sequência de comandos listada abaixo.

\small
\begin{lstlisting}
hadoop namenode -format
./sbin/start-dfs.sh
\end{lstlisting}
\normalsize

Para garantir que ele está rodando, pode ser executado o comando \mytexttt{hdfs 
dfsadmin -report}. Devem ser listados todos os \emph{datanodes} do ambiente.

Agora, mude de diretório para onde estão os CSVs que serão utilizados como 
entrada. Execute a sequência de comandos listada abaixo. Isso criará a pasta 
logs e a pasta inputs no HDFS e copiará todos os arquivos CSV para a pasta 
inputs. Este comando pode demorar.

\small
\begin{lstlisting}
hdfs dfs -mkdir -p logs
hdfs dfs -mkdir -p inputs
hdfs dfs -put * inputs/
\end{lstlisting}


Com o Hadoop rodando, iremos iniciar o Spark. Para isso, no nó mestre, execute 
o seguinte comando: \mytexttt{./sbin/start-master.sh}. Nos nós trabalhadores, o 
comando é um pouco diferente: \mytexttt{./sbin/start-slave.sh 
<IP\_MASTER>:7077}.

Se tudo ocorreu corretamente, agora temos o ambiente completo. Para executar o 
StarVZ, vá até sua pasta e execute o seguinte comando: 

\small
\begin{lstlisting}
./R/phase1-workflow.R -D /inputs <PATH\_PARA\_ENTITIES> 
spark://<IP\_MESTRE>:7077 cholesky
\end{lstlisting}
\normalsize

Este deve iniciar a aplicação em modo distribuído e rodar a aplicação 
utilizando o Spark.